---
title: 创建自定义语音 - 语音服务
titleSuffix: Azure Cognitive Services
description: 如果你已准备好上传数据，请转到自定义语音门户。 创建或选择一个自定义语音项目。 该项目必须与你打算用于语音训练的数据共享正确的语言/区域设置和性别属性。
services: cognitive-services
author: PatrickFarley
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 11/04/2019
ms.author: pafarley
ms.openlocfilehash: 1f444ca13224c27918812c12f0a9e86a50e0b994
ms.sourcegitcommit: f6e2ea5571e35b9ed3a79a22485eba4d20ae36cc
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 09/24/2021
ms.locfileid: "128644669"
---
# <a name="create-and-use-your-voice-model"></a>创建和使用语音模型

在[准备训练数据](how-to-custom-voice-prepare-data.md)中，你已了解可用于训练自定义神经语音的不同数据类型以及不同的格式要求。 准备好数据和配音员口述内容后，你便可以开始将其上传到 [Speech Studio](https://aka.ms/custom-voice-portal)。 本文介绍如何通过 Speech Studio 门户训练自定义神经语音。 请参阅自定义神经语音[支持的语言](language-support.md#customization)。

## <a name="prerequisites"></a>先决条件

* 完成[自定义神经语音入门](how-to-custom-voice.md)
* [准备训练数据](how-to-custom-voice-prepare-data.md)

## <a name="set-up-voice-talent"></a>设置配音员

配音员是个人或目标说话人，其语音会被录制下来并用于创建神经语音模型。 创建语音前，请定义语音角色并选择适当的配音员。 有关录制语音示例的详细信息，请参阅[教程](record-custom-voice-samples.md)。

若要训练神经语音，必须使用配音员录制的音频文件创建配音员配置文件，表示配音员同意授权使用其语音数据来训练自定义语音模型。 准备录制脚本时，请确保包含以下句子：

**“我[说出你的名字和姓氏]知道，[说出公司名称]将使用我的声音录音来创建和使用我的声音的合成版本。”**
这句话用于验证训练数据是否与同意声明中的音频相匹配。 > 在此处详细了解[配音员验证](/legal/cognitive-services/speech-service/custom-neural-voice/data-privacy-security-custom-neural-voice?context=%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext)。

> [!NOTE]
> 自定义神经语音只能在有限的访问权限下使用。 请务必了解[负责任 AI 的指导原则](/legal/cognitive-services/speech-service/custom-neural-voice/limited-access-custom-neural-voice?context=%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext)，然后[申请访问](https://aka.ms/customneural)。 

以下步骤假定你已准备好配音员口述同意文件。  转到 [Speech Studio](https://aka.ms/custom-voice-portal) 以选择自定义神经语音项目，然后按照以下步骤创建配音员配置文件。

1. 导航到“文本转语音” > “自定义语音” > “选择项目” > “设置配音员”   。

2. 选择“添加配音员”。

3. 接下来，为了定义语音特征，请选择要使用的“目标方案”。 然后描述语音特征。

> [!NOTE]
> 所提供的方案必须与应用程序表单中应用的方案一致。

4. 然后，转到“上传配音员口述内容”，按照说明上传事先准备好的配音员口述内容。

> [!NOTE]
> 请确保采用训练数据时的相同设置（包括录制环境和说话风格）记录口述内容。

5. 最后，转到“查看并创建”，可以查看设置，然后选择“提交” 。

## <a name="upload-your-data"></a>上传数据

准备好上传数据后，请转到“准备训练数据”选项卡，添加第一个训练集并上传数据。  训练集是一组音频言语及其映射脚本，用于训练语音模型。 可以使用训练集来整理训练数据。 数据就绪性检查将按照每个训练集执行。 可将多个数据导入一个训练集。

可以执行以下操作来创建和查看训练数据。

1. 在“准备训练数据”选项卡上，选择“添加训练集”以输入名称和说明，然后选择“创建”以添加新的训练集    。

   成功创建训练集后，便可以开始上传数据。 

2. 若要上传数据，请选择“上传数据” > “选择数据类型” > “上传数据”和“指定目标训练集”> 为数据输入名称和说明 > 查看设置并选择“提交”      。

> [!NOTE]
>- 在训练中将会删除重复的音频名称。 确保所选数据在该 .zip 文件或多个 .zip 文件中未包含相同的音频名称。 如果言语 ID（在音频或脚本文件中）重复，就会遭到拒绝。
>- 如果已在以前版本的 Speech Studio 中创建数据文件，则必须提前为数据指定训练集才能使用它们。 否则，数据名称后面将追加一个感叹号，而数据也无法使用。

上传的每个数据必须符合所选数据类型的要求。 在上传数据之正确设置数据格式非常重要，这可确保数据服务自定义神经语音准确处理数据。 转到[准备训练数据](how-to-custom-voice-prepare-data.md)，确保数据格式设置正确。

> [!NOTE]
> - 标准订阅 (S0) 用户可以同时上传五个数据文件。 如果达到限制，请先等待，直至至少其中一个数据文件导入完毕。 然后重试。
> - 每个订阅允许导入的数据文件的最大数目对于免费订阅 (F0) 用户而言为 10 个 .zip 文件；对于标准订阅 (S0) 用户而言，则为 500 个 .zip 文件。

点击“提交”按钮后，系统会自动验证数据文件。 数据验证包括针对音频文件执行一系列检查以验证文件格式、大小和采样率。 修复出现的任何错误，然后重新提交。 

上传数据后，可以在训练集详细信息视图中查看详细信息。 在“概述”选项卡上，可以进一步检查每个数据的发音分数和噪音级别。 发音评分范围为 0 到 100。 评分低于 70 通常表示语音错误或脚本不匹配。 口音重可能会降低发音分数，影响生成的数字语音。

信噪比 (SNR) 高表明音频中的噪音少。 通过专业录音棚录音通常可以达到 50+ 的 SNR。 音频的 SNR 低于 20 可能导致生成的语音中出现明显的噪音。

考虑重写录制发音分数低或信噪比不佳的表述。 如果无法重新录制，请考虑从数据中排除这些言语。

在“数据详细信息”中，可以检查训练集的数据详细信息。 如果数据出现任何典型问题，请先按照显示的消息中的说明进行操作以修复这些问题，然后再进行训练。

问题分为三种类型。 请参考以下三个表来检查相应错误类型。

请手动修复下表中列出的第一类错误，否则训练期间将排除包含这些错误的数据。

| 类别 | 名称 | 说明 |
| --------- | ----------- | --------------------------- |
| 脚本 | 分隔符无效| 必须使用制表符分隔言语 ID 和脚本内容。|
| 脚本 | 脚本 ID 无效| 脚本行 ID 必须是数字。|
| 脚本 | 重复的脚本|脚本内容的每一行必须唯一。 该行与 {} 重复。|
| 脚本 | 脚本太长| 脚本长度必须小于 1,000 个字符。|
| 脚本 | 没有匹配的音频| 每条言语（脚本文件的每一行）的 ID 必须与音频 ID 匹配。|
| 脚本 | 没有有效的脚本| 此数据集中找不到有效的脚本。 修复详细问题列表中显示的脚本行。|
| 音频 | 没有匹配的脚本| 没有任何音频文件与脚本 ID 匹配。 wav 文件的名称必须与脚本文件中的 ID 匹配。|
| 音频 | 音频格式无效| .wav 文件的音频格式无效。 使用 [SoX](http://sox.sourceforge.net/) 之类的音频工具检查 wav 文件格式。|
| 音频 | 采样率过低| .wav 文件的采样率不能低于 16 KHz。|
| 音频 | 音频太长| 音频时长超过 30 秒。 将长音频拆分为多个文件。 建议使用短于 15 秒的言语。|
| 音频 | 无有效音频| 在此数据集中找不到有效的音频。 检查音频数据，然后重新上传。|

表中列出的第二类错误将自动进行修复，但建议仔细检查已修复的数据。

| 类别 | 名称 | 说明 |
| --------- | ----------- | --------------------------- |
| 音频 | 立体声音频已自动修复 | 在音频示例录音中使用单声道。 立体声音频声道将自动合并为单声道，这可能会导致内容丢失。  下载规范化数据集并进行查看。|
| 数据量(Volume) | 音量峰值已自动修复 |音量峰值应在 -3 dB（最大音量的 70%）到 -6 dB (50%) 的范围内。 在录制示例或准备数据期间控制音量峰值。 此音频将线性地进行调整以自动适合峰值范围（-4 dB 或 65%）。 下载规范化数据集并进行查看。|
|不匹配 | 静音已自动修复| 检测到起始静音持续时间长于 200 毫秒，并已自动将其剪裁为 200 毫秒。 下载规范化数据集并进行查看。 |
| 不匹配 |静音已自动修复 | 检测到末尾静音持续时间长于 200 毫秒，并已自动将其剪裁为 200 毫秒。 下载规范化数据集并进行查看。 |
| 不匹配 |静音已自动修复 |检测到起始静音持续时间短于 100 毫秒，并已自动将其延长为 100 毫秒。 下载规范化数据集并进行查看。 |
| 不匹配 |静音已自动修复 | 检测到末尾静音持续时间短于 100 毫秒，并已自动将其延长为 100 毫秒。 下载规范化数据集并进行查看。|

如果下表中列出的第三类错误未得到修复，尽管训练期间不会排除存在这些错误的数据，但这些数据会影响训练质量。 如需较高质量的训练，建议手动修复这些错误。 

| 类别 | 名称 | 说明 |
| --------- | ----------- | --------------------------- |
| 脚本 | 非规范化文本|此脚本包含数字 0-9。 将其扩展为规范化字词，使之与音频匹配。 例如，将“123”规范化为“one hundred and twenty-three”。|
| 脚本 | 非规范化文本|此脚本包含符号 {}。 规范化这些符号，使之与音频匹配。 例如，将“50%”规范化为“fifty percent”。|
| 脚本 | 问题言语不足| 在所有言语中，至少应有 10% 的言语是疑问句。 这有助于语音模型正确表达疑问语调。|
| 脚本 |感叹言语不足| 在所有言语中，至少应有 10% 的言语是感叹句。 这有助于语音模型正确表达惊叹语调。|
| 音频| 神经语音采样率低 | 建议将 .wav 文件的采样率设置为 24 KHz 或更高，以创建神经语音。 否则，采样率将自动向上调整为 24 KHz。|
| 数据量(Volume) |总体音量过小|音量不应低于-18 dB（最大音量的 10%）。 在录制示例或准备数据期间，将音量平均水平控制在适当范围内。|
| 数据量(Volume) | 音量溢出| 在 {} 上检测到音量溢出。 调整录制设备，以避免音量在达到峰值时溢出。|
| 数据量(Volume) | 起始静音问题 | 前 100 毫秒静音不纯。 降低录制噪音最低水平，并将前 100 毫秒保持为起始静音。|
| 数据量(Volume)| 末尾静音问题| 最后 100 毫秒静音不纯。  降低录制噪音最低水平，并将最后 100 毫秒保持为末尾静音。|
| 不匹配 | 字词评分低|查看脚本和音频内容以确保它们匹配，并控制噪音最低水平。 缩短长时间静音的时长，或将过长的音频拆分为多个言语。|
| 不匹配 | 起始静音问题 |在第一个字词前面听到了额外的音频。 查看脚本和音频内容以确保它们匹配，控制噪音最低水平，并使前 100 毫秒保持静音。|
| 不匹配 | 末尾静音问题| 在最后一个字词后面听到了额外的音频。 查看脚本和音频内容以确保它们匹配，控制噪音最低水平，并使最后 100 毫秒保持静音。|
| 不匹配 | 信噪比低 | 音频 SNR 水平低于 20 dB。 建议至少设置为 35 dB。|
| 不匹配 | 无可用评分 |无法识别此音频中的语音内容。 检查音频和脚本内容，确保音频有效并与脚本匹配。|

## <a name="train-your-custom-neural-voice-model"></a>训练自定义神经语音模型

数据文件经过验证后，可以使用它们生成自定义神经语音模型。

1. 在“训练模型”选项卡上，选择“训练模型”以使用已上传的数据创建语音模型 。

2. 为模型和目标语言选择神经训练方法。

默认情况下，语音模式使用训练数据所采用的语言进行训练。 还可以选择为语音模型创建辅助语言（预览版）。  检查自定义神经语音和跨语言功能支持的语言：[用于自定义的语言](language-support.md#customization)。

训练自定义神经语音不免费。 有关详细信息，请查看[定价](https://azure.microsoft.com/pricing/details/cognitive-services/speech-services/)。 但是，如果你在 2021 年 3 月 31 日之前使用 S0 语音资源部署了统计参数或拼接声音模型，我们将为你的 Azure 订阅提供免费神经网络训练额度，你可以免费训练 5 个不同版本的神经语音。

3. 接下来，选择要用于训练的数据，并指定一个说话人文件。

>[!NOTE]
>- 你需要选择至少 300 个言语才能创建自定义神经语音。
>- 若要训练神经语音，必须指定配音员配置文件，其中包含配音员提供的语音同意文件，确认使用他/她的语音数据来训练自定义语音模型。 自定义神经语音只能在有限的访问权限下使用。 请确保你了解[负责任的 AI 使用原则](/legal/cognitive-services/speech-service/custom-neural-voice/limited-access-custom-neural-voice?context=%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext)并[在此处应用访问权限](https://aka.ms/customneural)。

4. 然后，选择测试脚本。

每次训练都会自动生成 100 个示例音频文件，以便你使用默认脚本来测试模型。 你还可以提供自己的测试脚本作为可选脚本。 测试脚本必须排除文件名（每条言语的 ID），否则会说出这些 ID。 以下示例演示如何在一个 .txt 文件中组织言语：

```
This is the waistline, and it's falling.
We have trouble scoring.
It was Janet Maslin.
```

每个言语段落生成一个单独的音频。 如果要将所有句子合并为一个音频，请将它们放在一个段落中。

>[!NOTE]
>- 测试脚本必须是小于 1 MB 的 txt 文件。 支持的编码格式包括 ANSI/ASCII、UTF-8、UTF-8-BOM、UTF-16-LE 或 UTF-16-BE。  
>- 生成的音频是上传的测试脚本和默认测试脚本的组合。

5. 请输入有助于识别此模型的名称和说明 。

请谨慎选择名称。 此处输入的名称将是在 SSML 输入过程中在请求中指定语音合成所需语音时使用的名称。 只允许字母、数字以及少量的标点字符，例如 -、_ 和 (', ')。 请对不同的神经语音模型使用不同名称。

通常使用“说明”字段来记录用于创建模型的数据的名称。

6. 查看设置，然后选择“提交”开始对模型进行训练。

> [!NOTE]
> 在训练中将会删除重复的音频名称。 确保你选择的数据在多个 .zip 文件中不包含相同的音频名称。

“模型”表会显示与新建模型相对应的新条目。 该表还会显示以下状态：“正在处理”、“成功”、“失败”。

显示的状态反映了将数据转换为语音模型的过程，如下所示。

| 状态 | 含义 |
| ----- | ------- |
| 正在处理 | 正在创建语音模型。 |
| 已成功 | 语音模型已创建并可部署。 |
| 已失败 | 语音模型在训练过程中由于多种原因（例如察觉不到的数据问题或网络问题）而失败。 |

训练持续时间因要训练的数据量而异。 训练自定义神经语音平均需要约 40 个计算小时。 

> [!NOTE]
> 标准订阅 (S0) 用户可以同时训练三个语音。 如果达到限制，请先等待，直至至少其中一种语音模型训练完毕，然后再试。 

7. 成功完成模型训练后，可以查看模型详细信息。

成功生成语音模型以后，可以使用生成的示例音频文件对其先测试后部署，然后就可以使用了。

语音质量取决于多种因素，包括训练数据的大小、录制内容的质量、脚本文件的准确性、训练数据中录制的语音与为预期用例设计的语音个性化匹配的程度，等等。 [查看此处以了解有关技术的功能和限制的详细信息，以及提高模型质量的最佳做法](/legal/cognitive-services/speech-service/custom-neural-voice/characteristics-and-limitations-custom-neural-voice?context=%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext)。 

## <a name="create-and-use-a-custom-neural-voice-endpoint"></a>创建并使用自定义语音终结点

成功创建并测试语音模型以后，即可在自定义的文本转语音终结点中部署它。 然后即可在通过 REST API 发出文本转语音请求时使用此终结点来替代常用的终结点。 只能通过部署模型时所用的订阅来调用自定义终结点。

可以执行以下操作来创建自定义神经语音终结点。

1. 在“部署模型”选项卡上，选择“部署模型” 。 
2. 接下来，为自定义终结点输入名称和说明 。
3. 然后，选择要与此终结点关联的自定义语音模型。 
4. 最后，选择“部署”以创建终结点。

单击“添加”按钮后，你会在终结点表中看到新终结点的条目。 新终结点的实例化可能需要数分钟。 当部署状态为“成功”时，终结点便可供使用。

如果你始终不使用终结点，则可以将其“挂起”并“继续”。 当终结点在挂起后重新激活时，终结点 URL 将保持不变，因此你无需在应用程序中更改代码。 

你还可以将终结点更新为新模型。 若要更改模型，请确保新模型的名称与要更新的模型相同。 

> [!NOTE]
>- 标准订阅 (S0) 用户可以创建多达 50 个终结点，每个终结点具有自身的自定义神经语音。
>- 若要使用自定义神经声音，必须指定语音模型名称，直接在 HTTP 请求中使用自定义 URI，并使用同一订阅通过 TTS 服务的身份验证。

部署终结点后，其名称将以链接的形式显示。 单击此链接可显示特定于该终结点的信息，例如终结点密钥、终结点 URL 和示例代码。

从功能上说，自定义终结点与用于文本转语音请求的标准终结点相同。  有关详细信息，请参阅[语音 SDK](./get-started-text-to-speech.md) 或 [REST API](rest-text-to-speech.md)。

我们还提供了一款联机工具（即[音频内容创建](https://speech.microsoft.com/audiocontentcreation)），让你可使用友好的 UI 微调其音频输出。

## <a name="next-steps"></a>后续步骤

- [如何录制语音示例](record-custom-voice-samples.md)
- [文本转语音 API 参考](rest-text-to-speech.md)
- [长音频 API](long-audio-api.md)
